[
    {
        "check_id": "3.1.1",
        "description": "Client certificate authentication should not be used for users (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be\nimplemented in place of client certificates.",
        "_source_file": "cis-1.11/controlplane.yaml"
    },
    {
        "check_id": "3.1.2",
        "description": "Service account token authentication should not be used for users (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented\nin place of service account tokens.",
        "_source_file": "cis-1.11/controlplane.yaml"
    },
    {
        "check_id": "3.1.3",
        "description": "Bootstrap token authentication should not be used for users (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented\nin place of bootstrap tokens.",
        "_source_file": "cis-1.11/controlplane.yaml"
    },
    {
        "check_id": "3.2.1",
        "description": "Ensure that a minimal audit policy is created (Manual)",
        "status": "FAIL",
        "reason": "['--audit-policy-file'] missing",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Create an audit policy file for your cluster.",
        "_source_file": "cis-1.11/controlplane.yaml"
    },
    {
        "check_id": "3.2.2",
        "description": "Ensure that the audit policy covers key security concerns (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Review the audit policy provided for the cluster and ensure that it covers\nat least the following areas,\n- Access to Secrets managed by the cluster. Care should be taken to only\n  log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in\n  order to avoid risk of logging sensitive data.\n- Modification of Pod and Deployment objects.\n- Use of `pods/exec`, `pods/portforward`, `pods/proxy` and `services/proxy`.\nFor most requests, minimally logging at the Metadata level is recommended\n(the most basic level of logging).",
        "_source_file": "cis-1.11/controlplane.yaml"
    },
    {
        "check_id": 2.1,
        "description": "Ensure that the --cert-file and --key-file arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--cert-file', 'ETCD_CERT_FILE']; Found ['--key-file', 'ETCD_KEY_FILE']",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Follow the etcd service documentation and configure TLS encryption.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml\non the master node and set the below parameters.\n--cert-file=</path/to/ca-file>\n--key-file=</path/to/key-file>",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.2,
        "description": "Ensure that the --cert-file and --key-file arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--cert-file', 'ETCD_CERT_FILE']; Found ['--key-file', 'ETCD_KEY_FILE']",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Follow the etcd service documentation and configure TLS encryption.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml\non the master node and set the below parameters.\n--cert-file=</path/to/ca-file>\n--key-file=</path/to/key-file>",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.3,
        "description": "Ensure that the --client-cert-auth argument is set to true (Automated)",
        "status": "FAIL",
        "reason": "['--client-cert-auth', 'ETCD_CLIENT_CERT_AUTH'] != True",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Edit the etcd pod specification file $etcdconf on the master\nnode and set the below parameter.\n--client-cert-auth=\"true\"",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.4,
        "description": "Ensure that the --auto-tls argument is not set to true (Automated)",
        "status": "PASS",
        "reason": "['--auto-tls', 'ETCD_AUTO_TLS'] correctly unset; ['--auto-tls', 'ETCD_AUTO_TLS'] != False",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Edit the etcd pod specification file $etcdconf on the master\nnode and either remove the --auto-tls parameter or set it to false.\n  --auto-tls=false",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.5,
        "description": "Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--peer-cert-file', 'ETCD_PEER_CERT_FILE']; Found ['--peer-key-file', 'ETCD_PEER_KEY_FILE']",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Follow the etcd service documentation and configure peer TLS encryption as appropriate\nfor your etcd cluster.\nThen, edit the etcd pod specification file $etcdconf on the\nmaster node and set the below parameters.\n--peer-client-file=</path/to/peer-cert-file>\n--peer-key-file=</path/to/peer-key-file>",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.6,
        "description": "Ensure that the --peer-client-cert-auth argument is set to true (Automated)",
        "status": "FAIL",
        "reason": "['--peer-client-cert-auth', 'ETCD_PEER_CLIENT_CERT_AUTH'] != True",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Edit the etcd pod specification file $etcdconf on the master\nnode and set the below parameter.\n--peer-client-cert-auth=true",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.7,
        "description": "Ensure that the --peer-auto-tls argument is not set to true (Automated)",
        "status": "PASS",
        "reason": "['--peer-auto-tls', 'ETCD_PEER_AUTO_TLS'] correctly unset; ['--peer-auto-tls', 'ETCD_PEER_AUTO_TLS'] != False",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "Edit the etcd pod specification file $etcdconf on the master\nnode and either remove the --peer-auto-tls parameter or set it to false.\n--peer-auto-tls=false",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": 2.8,
        "description": "Ensure that a unique Certificate Authority is used for etcd (Manual)",
        "status": "PASS",
        "reason": "Found ['--trusted-ca-file', 'ETCD_TRUSTED_CA_FILE']",
        "audit_command": "/bin/ps -ef | /bin/grep etcd | /bin/grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        3273    2912  2 09:15 ?        00:12:28 etcd --advertise-client-urls=https://10.0.2.15:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://10.0.2.15:2380 --initial-cluster=kubernetes=https://10.0.2.15:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.0.2.15:2380 --name=kubernetes --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt",
        "remediation": "[Manual test]\nFollow the etcd documentation and create a dedicated certificate authority setup for the\netcd service.\nThen, edit the etcd pod specification file $etcdconf on the\nmaster node and set the below parameter.\n--trusted-ca-file=</path/to/ca-file>",
        "_source_file": "cis-1.11/etcd.yaml"
    },
    {
        "check_id": "1.1.1",
        "description": "Ensure that the API server pod specification file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-apiserver.yaml; then stat -c permissions=%a /etc/kubernetes/manifests/kube-apiserver.yaml; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the\ncontrol plane node.\nFor example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.2",
        "description": "Ensure that the API server pod specification file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-apiserver.yaml; then stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.3",
        "description": "Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-controller-manager.yaml; then stat -c permissions=%a /etc/kubernetes/manifests/kube-controller-manager.yaml; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.4",
        "description": "Ensure that the controller manager pod specification file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-controller-manager.yaml; then stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.5",
        "description": "Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-scheduler.yaml; then stat -c permissions=%a /etc/kubernetes/manifests/kube-scheduler.yaml; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.6",
        "description": "Ensure that the scheduler pod specification file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/kube-scheduler.yaml; then stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.7",
        "description": "Ensure that the etcd pod specification file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/etcd.yaml; then find /etc/kubernetes/manifests/etcd.yaml -name '*etcd*' | xargs stat -c permissions=%a; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchmod 600 /etc/kubernetes/manifests/etcd.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.8",
        "description": "Ensure that the etcd pod specification file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/manifests/etcd.yaml; then find /etc/kubernetes/manifests/etcd.yaml -name '*etcd*' | xargs stat -c %U:%G; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchown root:root /etc/kubernetes/manifests/etcd.yaml",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.9",
        "description": "Ensure that the Container Network Interface file permissions are set to 600 or more restrictive (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "ps -ef | grep kubelet | grep -- --cni-conf-dir | sed 's%.*cni-conf-dir[= ]\\([^ ]*\\).*%\\1%' | xargs -I{} find {} -mindepth 1 | xargs --no-run-if-empty stat -c permissions=%a\nfind /var/lib/cni/networks -type f 2> /dev/null | xargs --no-run-if-empty stat -c permissions=%a\nfind /etc/cni/net.d -type f 2> /dev/null | xargs --no-run-if-empty stat -c permissions=%a\n",
        "audit_output": "permissions=644",
        "line_results": [
            {
                "line": "permissions=644",
                "status": "FAIL",
                "reason": "Permissions 0o644 > expected 0o600"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chmod 600 <path/to/cni/files>",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.10",
        "description": "Ensure that the Container Network Interface file ownership is set to root:root (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "ps -ef | grep kubelet | grep -- --cni-conf-dir | sed 's%.*cni-conf-dir[= ]\\([^ ]*\\).*%\\1%' | xargs -I{} find {} -mindepth 1 | xargs --no-run-if-empty stat -c %U:%G\nfind /var/lib/cni/networks -type f 2> /dev/null | xargs --no-run-if-empty stat -c %U:%G\nfind /etc/cni/net.d -type f 2> /dev/null | xargs --no-run-if-empty stat -c %U:%G\n",
        "audit_output": "root:root",
        "line_results": [
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchown root:root <path/to/cni/files>",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.11",
        "description": "Ensure that the etcd data directory permissions are set to 700 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o700 \u2264 0o700",
        "audit_command": "DATA_DIR=''\nfor d in $(ps -ef | grep etcd | grep -- --data-dir | sed 's%.*data-dir[= ]\\([^ ]*\\).*%\\1%'); do\n  if test -d \"$d\"; then DATA_DIR=\"$d\"; fi\ndone\nif ! test -d \"$DATA_DIR\"; then DATA_DIR=$etcddatadir; fi\nstat -c permissions=%a \"$DATA_DIR\"\n",
        "audit_output": "permissions=700",
        "remediation": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the command 'ps -ef | grep etcd'.\nRun the below command (based on the etcd data directory found above). For example,\nchmod 700 /var/lib/etcd",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.12",
        "description": "Ensure that the etcd data directory ownership is set to etcd:etcd (Automated)",
        "status": "FAIL",
        "reason": "Did not find ['etcd:etcd']",
        "audit_command": "DATA_DIR=''\nfor d in $(ps -ef | grep etcd | grep -- --data-dir | sed 's%.*data-dir[= ]\\([^ ]*\\).*%\\1%'); do\n  if test -d \"$d\"; then DATA_DIR=\"$d\"; fi\ndone\nif ! test -d \"$DATA_DIR\"; then DATA_DIR=$etcddatadir; fi\nstat -c %U:%G \"$DATA_DIR\"\n",
        "audit_output": "root:root",
        "remediation": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the command 'ps -ef | grep etcd'.\nRun the below command (based on the etcd data directory found above).\nFor example, chown etcd:etcd /var/lib/etcd",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.13",
        "description": "Ensure that the default administrative credential file permissions are set to 600 (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'for adminconf in /etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf; do if [ -e \"$adminconf\" ]; then stat -c \"permissions=%a %n\" \"$adminconf\"; fi; done'\n",
        "audit_output": "permissions=600 /etc/kubernetes/admin.conf\npermissions=600 /etc/kubernetes/super-admin.conf",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chmod 600 /etc/kubernetes/admin.conf\nOn Kubernetes 1.29+ the super-admin.conf file should also be modified, if present.\nFor example, chmod 600 /etc/kubernetes/super-admin.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.14",
        "description": "Ensure that the default administrative credential file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "/bin/sh -c 'for adminconf in /etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf; do if [ -e \"$adminconf\" ]; then stat -c \"ownership=%U:%G %n\" \"$adminconf\"; fi; done'\n",
        "audit_output": "ownership=root:root /etc/kubernetes/admin.conf\nownership=root:root /etc/kubernetes/super-admin.conf",
        "line_results": [
            {
                "line": "ownership=root:root /etc/kubernetes/admin.conf",
                "status": "PASS",
                "reason": "['ownership'] == root:root"
            },
            {
                "line": "ownership=root:root /etc/kubernetes/super-admin.conf",
                "status": "PASS",
                "reason": "['ownership'] == root:root"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example, chown root:root /etc/kubernetes/admin.conf\nOn Kubernetes 1.29+ the super-admin.conf file should also be modified, if present.\nFor example, chown root:root /etc/kubernetes/super-admin.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.15",
        "description": "Ensure that the scheduler.conf file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/scheduler.conf; then stat -c permissions=%a /etc/kubernetes/scheduler.conf; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchmod 600 /etc/kubernetes/scheduler.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.16",
        "description": "Ensure that the scheduler.conf file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/scheduler.conf; then stat -c %U:%G /etc/kubernetes/scheduler.conf; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchown root:root /etc/kubernetes/scheduler.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.17",
        "description": "Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive (Automated)",
        "status": "PASS",
        "reason": "Permissions 0o600 \u2264 0o600",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/controller-manager.conf; then stat -c permissions=%a /etc/kubernetes/controller-manager.conf; fi'",
        "audit_output": "permissions=600",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchmod 600 /etc/kubernetes/controller-manager.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.18",
        "description": "Ensure that the controller-manager.conf file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "Found ['root:root']",
        "audit_command": "/bin/sh -c 'if test -e /etc/kubernetes/controller-manager.conf; then stat -c %U:%G /etc/kubernetes/controller-manager.conf; fi'",
        "audit_output": "root:root",
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchown root:root /etc/kubernetes/controller-manager.conf",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.19",
        "description": "Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "find /etc/kubernetes/pki/ | xargs stat -c %U:%G",
        "audit_output": "root:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root\nroot:root",
        "line_results": [
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            },
            {
                "line": "root:root",
                "status": "PASS",
                "reason": "Found ['root:root']"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchown -R root:root /etc/kubernetes/pki/",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.20",
        "description": "Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "find /etc/kubernetes/pki/ -name '*.crt' | xargs stat -c permissions=%a",
        "audit_output": "permissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644\npermissions=644",
        "line_results": [
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            },
            {
                "line": "permissions=644",
                "status": "PASS",
                "reason": "Permissions 0o644 \u2264 0o644"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchmod -R 644 /etc/kubernetes/pki/*.crt",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.1.21",
        "description": "Ensure that the Kubernetes PKI key file permissions are set to 600 (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "find /etc/kubernetes/pki/ -name '*.key' | xargs stat -c permissions=%a",
        "audit_output": "permissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600\npermissions=600",
        "line_results": [
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            },
            {
                "line": "permissions=600",
                "status": "PASS",
                "reason": "Permissions 0o600 \u2264 0o600"
            }
        ],
        "remediation": "Run the below command (based on the file location on your system) on the control plane node.\nFor example,\nchmod -R 600 /etc/kubernetes/pki/*.key",
        "_source_file": "cis-1.11/master_1.yaml"
    },
    {
        "check_id": "1.2.1",
        "description": "Ensure that the --anonymous-auth argument is set to false (Manual)",
        "status": "FAIL",
        "reason": "['--anonymous-auth'] != False",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--anonymous-auth=false",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.2",
        "description": "Ensure that the --token-auth-file parameter is not set (Automated)",
        "status": "PASS",
        "reason": "['--token-auth-file'] correctly unset",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the documentation and configure alternate mechanisms for authentication. Then,\nedit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and remove the --token-auth-file=<filename> parameter.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.3",
        "description": "Ensure that the --DenyServiceExternalIPs is set (Manual)",
        "status": "FAIL",
        "reason": "['--enable-admission-plugins'] does not contain DenyServiceExternalIPs",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and add the `DenyServiceExternalIPs` plugin\nto the enabled admission plugins, as such --enable-admission-plugin=DenyServiceExternalIPs.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.4",
        "description": "Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--kubelet-client-certificate']; Found ['--kubelet-client-key']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the\napiserver and kubelets. Then, edit API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the control plane node and set the\nkubelet client certificate and key parameters as below.\n--kubelet-client-certificate=<path/to/client-certificate-file>\n--kubelet-client-key=<path/to/client-key-file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.5",
        "description": "Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)",
        "status": "FAIL",
        "reason": "Did not find ['--kubelet-certificate-authority']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and setup the TLS connection between\nthe apiserver and kubelets. Then, edit the API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the control plane node and set the\n--kubelet-certificate-authority parameter to the path to the cert file for the certificate authority.\n--kubelet-certificate-authority=<ca-string>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.6",
        "description": "Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)",
        "status": "PASS",
        "reason": "['--authorization-mode'] does not contain AlwaysAllow",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --authorization-mode parameter to values other than AlwaysAllow.\nOne such example could be as below.\n--authorization-mode=RBAC",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.7",
        "description": "Ensure that the --authorization-mode argument includes Node (Automated)",
        "status": "PASS",
        "reason": "['--authorization-mode'] contains Node",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --authorization-mode parameter to a value that includes Node.\n--authorization-mode=Node,RBAC",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.8",
        "description": "Ensure that the --authorization-mode argument includes RBAC (Automated)",
        "status": "PASS",
        "reason": "['--authorization-mode'] contains RBAC",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --authorization-mode parameter to a value that includes RBAC,\nfor example `--authorization-mode=Node,RBAC`.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.9",
        "description": "Ensure that the admission control plugin EventRateLimit is set (Manual)",
        "status": "FAIL",
        "reason": "['--enable-admission-plugins'] does not contain EventRateLimit",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set the desired limits in a configuration file.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\nand set the below parameters.\n--enable-admission-plugins=...,EventRateLimit,...\n--admission-control-config-file=<path/to/configuration/file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.10",
        "description": "Ensure that the admission control plugin AlwaysAdmit is not set (Automated)",
        "status": "PASS",
        "reason": "['--enable-admission-plugins'] does not contain AlwaysAdmit; ['--enable-admission-plugins'] should not be set",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and either remove the --enable-admission-plugins parameter, or set it to a\nvalue that does not include AlwaysAdmit.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.11",
        "description": "Ensure that the admission control plugin AlwaysPullImages is set (Manual)",
        "status": "FAIL",
        "reason": "['--enable-admission-plugins'] does not contain AlwaysPullImages",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --enable-admission-plugins parameter to include\nAlwaysPullImages.\n--enable-admission-plugins=...,AlwaysPullImages,...",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.12",
        "description": "Ensure that the admission control plugin ServiceAccount is set (Automated)",
        "status": "PASS",
        "reason": "['--disable-admission-plugins'] does not contain ServiceAccount; ['--disable-admission-plugins'] correctly unset",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the documentation and create ServiceAccount objects as per your environment.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and ensure that the --disable-admission-plugins parameter is set to a\nvalue that does not include ServiceAccount.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.13",
        "description": "Ensure that the admission control plugin NamespaceLifecycle is set (Automated)",
        "status": "PASS",
        "reason": "['--disable-admission-plugins'] does not contain NamespaceLifecycle; ['--disable-admission-plugins'] correctly unset",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --disable-admission-plugins parameter to\nensure it does not include NamespaceLifecycle.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.14",
        "description": "Ensure that the admission control plugin NodeRestriction is set (Automated)",
        "status": "PASS",
        "reason": "['--enable-admission-plugins'] contains NodeRestriction",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --enable-admission-plugins parameter to a\nvalue that includes NodeRestriction.\n--enable-admission-plugins=...,NodeRestriction,...",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.15",
        "description": "Ensure that the --profiling argument is set to false (Automated)",
        "status": "FAIL",
        "reason": "['--profiling'] != False",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--profiling=false",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.16",
        "description": "Ensure that the --audit-log-path argument is set (Automated)",
        "status": "FAIL",
        "reason": "Did not find ['--audit-log-path']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-path parameter to a suitable path and\nfile where you would like audit logs to be written, for example,\n--audit-log-path=/var/log/apiserver/audit.log",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.17",
        "description": "Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)",
        "status": "FAIL",
        "reason": "Could not parse numeric value for comparison or Target not found",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxage parameter to 30\nor as an appropriate number of days, for example,\n--audit-log-maxage=30",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.18",
        "description": "Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)",
        "status": "FAIL",
        "reason": "Could not parse numeric value for comparison or Target not found",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate\nvalue. For example,\n--audit-log-maxbackup=10",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.19",
        "description": "Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)",
        "status": "FAIL",
        "reason": "Could not parse numeric value for comparison or Target not found",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxsize parameter to an appropriate size in MB.\nFor example, to set it as 100 MB, --audit-log-maxsize=100",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.20",
        "description": "Ensure that the --request-timeout argument is set as appropriate (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\nand set the below parameter as appropriate and if needed.\nFor example, --request-timeout=300s",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.21",
        "description": "Ensure that the --service-account-lookup argument is set to true (Automated)",
        "status": "PASS",
        "reason": "['--service-account-lookup'] correctly unset; ['--service-account-lookup'] != True",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--service-account-lookup=true\nAlternatively, you can delete the --service-account-lookup parameter from this file so\nthat the default takes effect.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.22",
        "description": "Ensure that the --service-account-key-file argument is set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--service-account-key-file']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --service-account-key-file parameter\nto the public key file for service accounts. For example,\n--service-account-key-file=<filename>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.23",
        "description": "Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--etcd-certfile']; Found ['--etcd-keyfile']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the etcd certificate and key file parameters.\n--etcd-certfile=<path/to/client-certificate-file>\n--etcd-keyfile=<path/to/client-key-file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.24",
        "description": "Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--tls-cert-file']; Found ['--tls-private-key-file']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection on the apiserver.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the TLS certificate and private key file parameters.\n--tls-cert-file=<path/to/tls-certificate-file>\n--tls-private-key-file=<path/to/tls-key-file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.25",
        "description": "Ensure that the --client-ca-file argument is set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--client-ca-file']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection on the apiserver.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the client certificate authority file.\n--client-ca-file=<path/to/client-ca-file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.26",
        "description": "Ensure that the --etcd-cafile argument is set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--etcd-cafile']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the etcd certificate authority file parameter.\n--etcd-cafile=<path/to/ca-file>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.27",
        "description": "Ensure that the --encryption-provider-config argument is set as appropriate (Manual)",
        "status": "FAIL",
        "reason": "Did not find ['--encryption-provider-config']",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Follow the Kubernetes documentation and configure a EncryptionConfig file.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --encryption-provider-config parameter to the path of that file.\nFor example, --encryption-provider-config=</path/to/EncryptionConfig/File>",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.28",
        "description": "Ensure that encryption providers are appropriately configured (Manual)",
        "status": "WARN",
        "reason": "No output from audit command, the recommendation might be manual",
        "audit_command": "ENCRYPTION_PROVIDER_CONFIG=$(ps -ef | grep kube-apiserver | grep -- --encryption-provider-config | sed 's%.*encryption-provider-config[= ]\\([^ ]*\\).*%\\1%')\nif test -e $ENCRYPTION_PROVIDER_CONFIG; then grep -A1 'providers:' $ENCRYPTION_PROVIDER_CONFIG | tail -n1 | grep -o \"[A-Za-z]*\" | sed 's/^/provider=/'; fi\n",
        "audit_output": "",
        "remediation": "Follow the Kubernetes documentation and configure a EncryptionConfig file.\nIn this file, choose aescbc, kms or secretbox as the encryption provider.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.29",
        "description": "Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Manual)",
        "status": "WARN",
        "reason": "Could not parse values for valid_elements",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,\nTLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\nTLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\nTLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,\nTLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.2.30",
        "description": "Ensure that the --service-account-extend-token-expiration parameter is set to false (Automated)",
        "status": "FAIL",
        "reason": "['--service-account-extend-token-expiration'] != False",
        "audit_command": "/bin/ps -ef | grep kube-apiserver | grep -v grep",
        "audit_output": "root        3240    2901  4 09:15 ?        00:23:08 kube-apiserver --advertise-address=10.0.2.15 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --service-account-extend-token-expiration parameter to false.\n`--service-account-extend-token-expiration=false`\nBy default, this parameter is set to true.",
        "_source_file": "cis-1.11/master_2.yaml"
    },
    {
        "check_id": "1.3.1",
        "description": "Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Manual)",
        "status": "FAIL",
        "reason": "Did not find ['--terminated-pod-gc-threshold']",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and set the --terminated-pod-gc-threshold to an appropriate threshold,\nfor example, --terminated-pod-gc-threshold=10",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.2",
        "description": "Ensure that the --profiling argument is set to false (Automated)",
        "status": "FAIL",
        "reason": "['--profiling'] != False",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and set the below parameter.\n--profiling=false",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.3",
        "description": "Ensure that the --use-service-account-credentials argument is set to true (Automated)",
        "status": "FAIL",
        "reason": "['--use-service-account-credentials'] != True",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node to set the below parameter.\n--use-service-account-credentials=true",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.4",
        "description": "Ensure that the --service-account-private-key-file argument is set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--service-account-private-key-file']",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and set the --service-account-private-key-file parameter\nto the private key file for service accounts.\n--service-account-private-key-file=<filename>",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.5",
        "description": "Ensure that the --root-ca-file argument is set as appropriate (Automated)",
        "status": "PASS",
        "reason": "Found ['--root-ca-file']",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and set the --root-ca-file parameter to the certificate bundle file`.\n--root-ca-file=<path/to/file>",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.6",
        "description": "Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)",
        "status": "PASS",
        "reason": "['--feature-gates'] missing; ['--feature-gates'] correctly unset",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true.\n--feature-gates=RotateKubeletServerCertificate=true",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.3.7",
        "description": "Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)",
        "status": "PASS",
        "reason": "['--bind-address'] == 127.0.0.1; ['--bind-address'] should not be set",
        "audit_command": "/bin/ps -ef | grep kube-controller-manager | grep -v grep",
        "audit_output": "root        3251    2911  1 09:15 ?        00:07:53 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true",
        "remediation": "Edit the Controller Manager pod specification file $controllermanagerconf\non the control plane node and ensure the correct value for the --bind-address parameter",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.4.1",
        "description": "Ensure that the --profiling argument is set to false (Automated)",
        "status": "FAIL",
        "reason": "['--profiling'] != False",
        "audit_command": "/bin/ps -ef | grep kube-scheduler | grep -v grep",
        "audit_output": "root        3185    2902  0 09:15 ?        00:01:40 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true",
        "remediation": "Edit the Scheduler pod specification file $schedulerconf file\non the control plane node and set the below parameter.\n--profiling=false",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "1.4.2",
        "description": "Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)",
        "status": "PASS",
        "reason": "['--bind-address'] == 127.0.0.1; ['--bind-address'] should not be set",
        "audit_command": "/bin/ps -ef | grep kube-scheduler | grep -v grep",
        "audit_output": "root        3185    2902  0 09:15 ?        00:01:40 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true",
        "remediation": "Edit the Scheduler pod specification file $schedulerconf\non the control plane node and ensure the correct value for the --bind-address parameter",
        "_source_file": "cis-1.11/master_3.yaml"
    },
    {
        "check_id": "5.1.1",
        "description": "Ensure that the cluster-admin role is only used where required (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name --no-headers | while read -r role_name role_binding subject\ndo\n  if [[ \"${role_name}\" != \"cluster-admin\" && \"${role_binding}\" == \"cluster-admin\" ]]; then\n    is_compliant=\"False\"\n  else\n    is_compliant=\"True\"\n  fi;\n  echo \"**role_name: ${role_name} role_binding: ${role_binding} subject: ${subject} is_compliant:${is_compliant}\"\ndone\n",
        "audit_output": "**role_name: audit-runner-binding role_binding: cluster-admin subject: audit-runner is_compliant:True\n**role_name: audit-runner-impersonate-binding role_binding: audit-runner-impersonate subject: audit-runner is_compliant:True\n**role_name: cluster-admin role_binding: cluster-admin subject: system:masters is_compliant:True\n**role_name: flannel role_binding: flannel subject: flannel is_compliant:True\n**role_name: kubeadm:cluster-admins role_binding: cluster-admin subject: kubeadm:cluster-admins is_compliant:True\n**role_name: kubeadm:get-nodes role_binding: kubeadm:get-nodes subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True\n**role_name: kubeadm:kubelet-bootstrap role_binding: system:node-bootstrapper subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True\n**role_name: kubeadm:node-autoapprove-bootstrap role_binding: system:certificates.k8s.io:certificatesigningrequests:nodeclient subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True\n**role_name: kubeadm:node-autoapprove-certificate-rotation role_binding: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient subject: system:nodes is_compliant:True\n**role_name: kubeadm:node-proxier role_binding: system:node-proxier subject: kube-proxy is_compliant:True\n**role_name: system:basic-user role_binding: system:basic-user subject: system:authenticated is_compliant:True\n**role_name: system:controller:attachdetach-controller role_binding: system:controller:attachdetach-controller subject: attachdetach-controller is_compliant:True\n**role_name: system:controller:certificate-controller role_binding: system:controller:certificate-controller subject: certificate-controller is_compliant:True\n**role_name: system:controller:clusterrole-aggregation-controller role_binding: system:controller:clusterrole-aggregation-controller subject: clusterrole-aggregation-controller is_compliant:True\n**role_name: system:controller:cronjob-controller role_binding: system:controller:cronjob-controller subject: cronjob-controller is_compliant:True\n**role_name: system:controller:daemon-set-controller role_binding: system:controller:daemon-set-controller subject: daemon-set-controller is_compliant:True\n**role_name: system:controller:deployment-controller role_binding: system:controller:deployment-controller subject: deployment-controller is_compliant:True\n**role_name: system:controller:disruption-controller role_binding: system:controller:disruption-controller subject: disruption-controller is_compliant:True\n**role_name: system:controller:endpoint-controller role_binding: system:controller:endpoint-controller subject: endpoint-controller is_compliant:True\n**role_name: system:controller:endpointslice-controller role_binding: system:controller:endpointslice-controller subject: endpointslice-controller is_compliant:True\n**role_name: system:controller:endpointslicemirroring-controller role_binding: system:controller:endpointslicemirroring-controller subject: endpointslicemirroring-controller is_compliant:True\n**role_name: system:controller:ephemeral-volume-controller role_binding: system:controller:ephemeral-volume-controller subject: ephemeral-volume-controller is_compliant:True\n**role_name: system:controller:expand-controller role_binding: system:controller:expand-controller subject: expand-controller is_compliant:True\n**role_name: system:controller:generic-garbage-collector role_binding: system:controller:generic-garbage-collector subject: generic-garbage-collector is_compliant:True\n**role_name: system:controller:horizontal-pod-autoscaler role_binding: system:controller:horizontal-pod-autoscaler subject: horizontal-pod-autoscaler is_compliant:True\n**role_name: system:controller:job-controller role_binding: system:controller:job-controller subject: job-controller is_compliant:True\n**role_name: system:controller:legacy-service-account-token-cleaner role_binding: system:controller:legacy-service-account-token-cleaner subject: legacy-service-account-token-cleaner is_compliant:True\n**role_name: system:controller:namespace-controller role_binding: system:controller:namespace-controller subject: namespace-controller is_compliant:True\n**role_name: system:controller:node-controller role_binding: system:controller:node-controller subject: node-controller is_compliant:True\n**role_name: system:controller:persistent-volume-binder role_binding: system:controller:persistent-volume-binder subject: persistent-volume-binder is_compliant:True\n**role_name: system:controller:pod-garbage-collector role_binding: system:controller:pod-garbage-collector subject: pod-garbage-collector is_compliant:True\n**role_name: system:controller:pv-protection-controller role_binding: system:controller:pv-protection-controller subject: pv-protection-controller is_compliant:True\n**role_name: system:controller:pvc-protection-controller role_binding: system:controller:pvc-protection-controller subject: pvc-protection-controller is_compliant:True\n**role_name: system:controller:replicaset-controller role_binding: system:controller:replicaset-controller subject: replicaset-controller is_compliant:True\n**role_name: system:controller:replication-controller role_binding: system:controller:replication-controller subject: replication-controller is_compliant:True\n**role_name: system:controller:resourcequota-controller role_binding: system:controller:resourcequota-controller subject: resourcequota-controller is_compliant:True\n**role_name: system:controller:root-ca-cert-publisher role_binding: system:controller:root-ca-cert-publisher subject: root-ca-cert-publisher is_compliant:True\n**role_name: system:controller:route-controller role_binding: system:controller:route-controller subject: route-controller is_compliant:True\n**role_name: system:controller:service-account-controller role_binding: system:controller:service-account-controller subject: service-account-controller is_compliant:True\n**role_name: system:controller:service-controller role_binding: system:controller:service-controller subject: service-controller is_compliant:True\n**role_name: system:controller:statefulset-controller role_binding: system:controller:statefulset-controller subject: statefulset-controller is_compliant:True\n**role_name: system:controller:ttl-after-finished-controller role_binding: system:controller:ttl-after-finished-controller subject: ttl-after-finished-controller is_compliant:True\n**role_name: system:controller:ttl-controller role_binding: system:controller:ttl-controller subject: ttl-controller is_compliant:True\n**role_name: system:controller:validatingadmissionpolicy-status-controller role_binding: system:controller:validatingadmissionpolicy-status-controller subject: validatingadmissionpolicy-status-controller is_compliant:True\n**role_name: system:coredns role_binding: system:coredns subject: coredns is_compliant:True\n**role_name: system:discovery role_binding: system:discovery subject: system:authenticated is_compliant:True\n**role_name: system:kube-controller-manager role_binding: system:kube-controller-manager subject: system:kube-controller-manager is_compliant:True\n**role_name: system:kube-dns role_binding: system:kube-dns subject: kube-dns is_compliant:True\n**role_name: system:kube-scheduler role_binding: system:kube-scheduler subject: system:kube-scheduler is_compliant:True\n**role_name: system:monitoring role_binding: system:monitoring subject: system:monitoring is_compliant:True\n**role_name: system:node role_binding: system:node subject: <none> is_compliant:True\n**role_name: system:node-proxier role_binding: system:node-proxier subject: system:kube-proxy is_compliant:True\n**role_name: system:public-info-viewer role_binding: system:public-info-viewer subject: system:authenticated,system:unauthenticated is_compliant:True\n**role_name: system:service-account-issuer-discovery role_binding: system:service-account-issuer-discovery subject: system:serviceaccounts is_compliant:True\n**role_name: system:volume-scheduler role_binding: system:volume-scheduler subject: system:kube-scheduler is_compliant:True",
        "line_results": [
            {
                "line": "**role_name: audit-runner-binding role_binding: cluster-admin subject: audit-runner is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: audit-runner-impersonate-binding role_binding: audit-runner-impersonate subject: audit-runner is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: cluster-admin role_binding: cluster-admin subject: system:masters is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: flannel role_binding: flannel subject: flannel is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:cluster-admins role_binding: cluster-admin subject: kubeadm:cluster-admins is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:get-nodes role_binding: kubeadm:get-nodes subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:kubelet-bootstrap role_binding: system:node-bootstrapper subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:node-autoapprove-bootstrap role_binding: system:certificates.k8s.io:certificatesigningrequests:nodeclient subject: system:bootstrappers:kubeadm:default-node-token is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:node-autoapprove-certificate-rotation role_binding: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient subject: system:nodes is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: kubeadm:node-proxier role_binding: system:node-proxier subject: kube-proxy is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:basic-user role_binding: system:basic-user subject: system:authenticated is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:attachdetach-controller role_binding: system:controller:attachdetach-controller subject: attachdetach-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:certificate-controller role_binding: system:controller:certificate-controller subject: certificate-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:clusterrole-aggregation-controller role_binding: system:controller:clusterrole-aggregation-controller subject: clusterrole-aggregation-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:cronjob-controller role_binding: system:controller:cronjob-controller subject: cronjob-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:daemon-set-controller role_binding: system:controller:daemon-set-controller subject: daemon-set-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:deployment-controller role_binding: system:controller:deployment-controller subject: deployment-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:disruption-controller role_binding: system:controller:disruption-controller subject: disruption-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:endpoint-controller role_binding: system:controller:endpoint-controller subject: endpoint-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:endpointslice-controller role_binding: system:controller:endpointslice-controller subject: endpointslice-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:endpointslicemirroring-controller role_binding: system:controller:endpointslicemirroring-controller subject: endpointslicemirroring-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:ephemeral-volume-controller role_binding: system:controller:ephemeral-volume-controller subject: ephemeral-volume-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:expand-controller role_binding: system:controller:expand-controller subject: expand-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:generic-garbage-collector role_binding: system:controller:generic-garbage-collector subject: generic-garbage-collector is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:horizontal-pod-autoscaler role_binding: system:controller:horizontal-pod-autoscaler subject: horizontal-pod-autoscaler is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:job-controller role_binding: system:controller:job-controller subject: job-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:legacy-service-account-token-cleaner role_binding: system:controller:legacy-service-account-token-cleaner subject: legacy-service-account-token-cleaner is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:namespace-controller role_binding: system:controller:namespace-controller subject: namespace-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:node-controller role_binding: system:controller:node-controller subject: node-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:persistent-volume-binder role_binding: system:controller:persistent-volume-binder subject: persistent-volume-binder is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:pod-garbage-collector role_binding: system:controller:pod-garbage-collector subject: pod-garbage-collector is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:pv-protection-controller role_binding: system:controller:pv-protection-controller subject: pv-protection-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:pvc-protection-controller role_binding: system:controller:pvc-protection-controller subject: pvc-protection-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:replicaset-controller role_binding: system:controller:replicaset-controller subject: replicaset-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:replication-controller role_binding: system:controller:replication-controller subject: replication-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:resourcequota-controller role_binding: system:controller:resourcequota-controller subject: resourcequota-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:root-ca-cert-publisher role_binding: system:controller:root-ca-cert-publisher subject: root-ca-cert-publisher is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:route-controller role_binding: system:controller:route-controller subject: route-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:service-account-controller role_binding: system:controller:service-account-controller subject: service-account-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:service-controller role_binding: system:controller:service-controller subject: service-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:statefulset-controller role_binding: system:controller:statefulset-controller subject: statefulset-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:ttl-after-finished-controller role_binding: system:controller:ttl-after-finished-controller subject: ttl-after-finished-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:ttl-controller role_binding: system:controller:ttl-controller subject: ttl-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:controller:validatingadmissionpolicy-status-controller role_binding: system:controller:validatingadmissionpolicy-status-controller subject: validatingadmissionpolicy-status-controller is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:coredns role_binding: system:coredns subject: coredns is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:discovery role_binding: system:discovery subject: system:authenticated is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:kube-controller-manager role_binding: system:kube-controller-manager subject: system:kube-controller-manager is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:kube-dns role_binding: system:kube-dns subject: kube-dns is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:kube-scheduler role_binding: system:kube-scheduler subject: system:kube-scheduler is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:monitoring role_binding: system:monitoring subject: system:monitoring is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:node role_binding: system:node subject: <none> is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:node-proxier role_binding: system:node-proxier subject: system:kube-proxy is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:public-info-viewer role_binding: system:public-info-viewer subject: system:authenticated,system:unauthenticated is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:service-account-issuer-discovery role_binding: system:service-account-issuer-discovery subject: system:serviceaccounts is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "**role_name: system:volume-scheduler role_binding: system:volume-scheduler subject: system:kube-scheduler is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and\nif they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]\nCondition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.2",
        "description": "Minimize access to secrets (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove get, list and watch access to Secret objects in the cluster.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.3",
        "description": "Minimize wildcard use in Roles and ClusterRoles (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "# Check Roles\nkubectl get roles --all-namespaces -o custom-columns=ROLE_NAMESPACE:.metadata.namespace,ROLE_NAME:.metadata.name --no-headers | while read -r role_namespace role_name\ndo\n  role_rules=$(kubectl get role -n \"${role_namespace}\" \"${role_name}\" -o=json | jq -c '.rules')\n  if echo \"${role_rules}\" | grep -q \"\\[\\\"\\*\\\"\\]\"; then\n    role_is_compliant=\"False\"\n  else\n    role_is_compliant=\"True\"\n  fi;\n  echo \"**role_name: ${role_name} role_namespace: ${role_namespace} role_rules: ${role_rules} role_is_compliant: ${role_is_compliant}\"\ndone\n\n# Check ClusterRoles\nkubectl get clusterroles -o custom-columns=CLUSTERROLE_NAME:.metadata.name --no-headers | while read -r clusterrole_name\ndo\n  clusterrole_rules=$(kubectl get clusterrole \"${clusterrole_name}\" -o=json | jq -c '.rules')\n  if echo \"${clusterrole_rules}\" | grep -q \"\\[\\\"\\*\\\"\\]\"; then\n    clusterrole_is_compliant=\"False\"\n  else\n    clusterrole_is_compliant=\"True\"\n  fi;\necho \"**clusterrole_name: ${clusterrole_name} clusterrole_rules: ${clusterrole_rules} clusterrole_is_compliant: ${clusterrole_is_compliant}\"\ndone\n",
        "audit_output": "**role_name: kubeadm:bootstrap-signer-clusterinfo role_namespace: kube-public role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"cluster-info\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True\n**role_name: system:controller:bootstrap-signer role_namespace: kube-public role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"cluster-info\"],\"resources\":[\"configmaps\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] role_is_compliant: True\n**role_name: extension-apiserver-authentication-reader role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"extension-apiserver-authentication\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] role_is_compliant: True\n**role_name: kube-proxy role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-proxy\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True\n**role_name: kubeadm:kubelet-config role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kubelet-config\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True\n**role_name: kubeadm:nodes-kubeadm-config role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kubeadm-config\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True\n**role_name: system::leader-locking-kube-controller-manager role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-controller-manager\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"update\"]}] role_is_compliant: True\n**role_name: system::leader-locking-kube-scheduler role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-scheduler\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"update\"]}] role_is_compliant: True\n**role_name: system:controller:bootstrap-signer role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] role_is_compliant: True\n**role_name: system:controller:cloud-provider role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]}] role_is_compliant: True\n**role_name: system:controller:token-cleaner role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] role_is_compliant: True\n**clusterrole_name: admin clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"rolebindings\",\"roles\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: audit-runner-impersonate clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"users\",\"groups\",\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"selfsubjectaccessreviews\",\"selfsubjectrulesreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True\n**clusterrole_name: cluster-admin clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"*\"]},{\"nonResourceURLs\":[\"*\"],\"verbs\":[\"*\"]}] clusterrole_is_compliant: False\n**clusterrole_name: edit clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: flannel clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: kubeadm:get-nodes clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:aggregate-to-admin clusterrole_rules: [{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"rolebindings\",\"roles\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:aggregate-to-edit clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:aggregate-to-view clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:auth-delegator clusterrole_rules: [{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:basic-user clusterrole_rules: [{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"selfsubjectaccessreviews\",\"selfsubjectrulesreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"selfsubjectreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:certificatesigningrequests:nodeclient clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/nodeclient\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/selfnodeclient\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:kube-apiserver-client-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:kube-apiserver-client-kubelet-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client-kubelet\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:kubelet-serving-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kubelet-serving\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:certificates.k8s.io:legacy-unknown-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/legacy-unknown\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:attachdetach-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"volumeattachments\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:certificate-controller clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/approval\",\"certificatesigningrequests/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client-kubelet\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client\",\"kubernetes.io/kube-apiserver-client-kubelet\",\"kubernetes.io/kubelet-serving\",\"kubernetes.io/legacy-unknown\"],\"resources\":[\"signers\"],\"verbs\":[\"sign\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:clusterrole-aggregation-controller clusterrole_rules: [{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"clusterroles\"],\"verbs\":[\"escalate\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:cronjob-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:daemon-set-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/binding\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:deployment-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:disruption-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*/scale\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:controller:endpoint-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints/restricted\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:endpointslice-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\",\"pods\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:endpointslicemirroring-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:ephemeral-volume-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:expand-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:generic-garbage-collector clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:controller:horizontal-pod-autoscaler clusterrole_rules: [{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*/scale\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\"]},{\"apiGroups\":[\"metrics.k8s.io\"],\"resources\":[\"pods\"],\"verbs\":[\"list\"]},{\"apiGroups\":[\"custom.metrics.k8s.io\"],\"resources\":[\"*\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"external.metrics.k8s.io\"],\"resources\":[\"*\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:controller:job-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:legacy-service-account-token-cleaner clusterrole_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-apiserver-legacy-service-account-token-tracking\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\",\"patch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:namespace-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces/finalize\",\"namespaces/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"delete\",\"deletecollection\",\"get\",\"list\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:controller:node-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"delete\",\"get\",\"list\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:persistent-volume-binder clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"create\",\"delete\",\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"create\",\"delete\",\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:pod-garbage-collector clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:pv-protection-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:pvc-protection-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:replicaset-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:replication-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:resourcequota-controller clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"resourcequotas/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:controller:root-ca-cert-publisher clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"create\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:route-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:service-account-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:service-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:statefulset-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"delete\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/finalizers\"],\"verbs\":[\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:ttl-after-finished-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:ttl-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:controller:validatingadmissionpolicy-status-controller clusterrole_rules: [{\"apiGroups\":[\"admissionregistration.k8s.io\"],\"resources\":[\"validatingadmissionpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"admissionregistration.k8s.io\"],\"resources\":[\"validatingadmissionpolicies/status\"],\"verbs\":[\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:coredns clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\",\"pods\",\"namespaces\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:discovery clusterrole_rules: [{\"nonResourceURLs\":[\"/api\",\"/api/*\",\"/apis\",\"/apis/*\",\"/healthz\",\"/livez\",\"/openapi\",\"/openapi/*\",\"/readyz\",\"/version\",\"/version/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:heapster clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"events\",\"namespaces\",\"nodes\",\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:kube-aggregator clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:kube-controller-manager clusterrole_rules: [{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resourceNames\":[\"kube-controller-manager\"],\"resources\":[\"leases\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\",\"serviceaccounts\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"namespaces\",\"secrets\",\"serviceaccounts\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\",\"serviceaccounts\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:kube-dns clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:kube-scheduler clusterrole_rules: [{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resourceNames\":[\"kube-scheduler\"],\"resources\":[\"leases\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"pods/binding\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csistoragecapacities\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:kubelet-api-admin clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"proxy\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/log\",\"nodes/metrics\",\"nodes/proxy\",\"nodes/stats\"],\"verbs\":[\"*\"]}] clusterrole_is_compliant: False\n**clusterrole_name: system:monitoring clusterrole_rules: [{\"nonResourceURLs\":[\"/healthz\",\"/healthz/*\",\"/livez\",\"/livez/*\",\"/metrics\",\"/metrics/slis\",\"/readyz\",\"/readyz/*\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:node clusterrole_rules: [{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\",\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"secrets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"volumeattachments\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"node.k8s.io\"],\"resources\":[\"runtimeclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:node-bootstrapper clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:node-problem-detector clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:node-proxier clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:persistent-volume-provisioner clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:public-info-viewer clusterrole_rules: [{\"nonResourceURLs\":[\"/healthz\",\"/livez\",\"/readyz\",\"/version\",\"/version/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:service-account-issuer-discovery clusterrole_rules: [{\"nonResourceURLs\":[\"/.well-known/openid-configuration\",\"/.well-known/openid-configuration/\",\"/openid/v1/jwks\",\"/openid/v1/jwks/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True\n**clusterrole_name: system:volume-scheduler clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True\n**clusterrole_name: view clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
        "line_results": [
            {
                "line": "**role_name: kubeadm:bootstrap-signer-clusterinfo role_namespace: kube-public role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"cluster-info\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system:controller:bootstrap-signer role_namespace: kube-public role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"cluster-info\"],\"resources\":[\"configmaps\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: extension-apiserver-authentication-reader role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"extension-apiserver-authentication\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: kube-proxy role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-proxy\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: kubeadm:kubelet-config role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kubelet-config\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: kubeadm:nodes-kubeadm-config role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kubeadm-config\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system::leader-locking-kube-controller-manager role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-controller-manager\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"update\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system::leader-locking-kube-scheduler role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-scheduler\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\",\"update\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system:controller:bootstrap-signer role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system:controller:cloud-provider role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**role_name: system:controller:token-cleaner role_namespace: kube-system role_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] role_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] missing"
            },
            {
                "line": "**clusterrole_name: admin clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"rolebindings\",\"roles\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: audit-runner-impersonate clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"users\",\"groups\",\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"selfsubjectaccessreviews\",\"selfsubjectrulesreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: cluster-admin clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"*\"]},{\"nonResourceURLs\":[\"*\"],\"verbs\":[\"*\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: edit clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: flannel clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: kubeadm:get-nodes clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:aggregate-to-admin clusterrole_rules: [{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"rolebindings\",\"roles\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:aggregate-to-edit clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\",\"secrets\",\"services/proxy\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"impersonate\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"pods/attach\",\"pods/exec\",\"pods/portforward\",\"pods/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"events\",\"persistentvolumeclaims\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"secrets\",\"serviceaccounts\",\"services\",\"services/proxy\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"replicasets\",\"replicasets/scale\",\"statefulsets\",\"statefulsets/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"jobs\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"deployments\",\"deployments/rollback\",\"deployments/scale\",\"ingresses\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicationcontrollers/scale\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"networkpolicies\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:aggregate-to-view clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:auth-delegator clusterrole_rules: [{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:basic-user clusterrole_rules: [{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"selfsubjectaccessreviews\",\"selfsubjectrulesreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"selfsubjectreviews\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:certificatesigningrequests:nodeclient clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/nodeclient\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/selfnodeclient\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:kube-apiserver-client-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:kube-apiserver-client-kubelet-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client-kubelet\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:kubelet-serving-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kubelet-serving\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:certificates.k8s.io:legacy-unknown-approver clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/legacy-unknown\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:attachdetach-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"volumeattachments\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:certificate-controller clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests/approval\",\"certificatesigningrequests/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client-kubelet\"],\"resources\":[\"signers\"],\"verbs\":[\"approve\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resourceNames\":[\"kubernetes.io/kube-apiserver-client\",\"kubernetes.io/kube-apiserver-client-kubelet\",\"kubernetes.io/kubelet-serving\",\"kubernetes.io/legacy-unknown\"],\"resources\":[\"signers\"],\"verbs\":[\"sign\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:clusterrole-aggregation-controller clusterrole_rules: [{\"apiGroups\":[\"rbac.authorization.k8s.io\"],\"resources\":[\"clusterroles\"],\"verbs\":[\"escalate\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:cronjob-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:daemon-set-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"daemonsets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/binding\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:deployment-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:disruption-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*/scale\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:endpoint-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints/restricted\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:endpointslice-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\",\"pods\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:endpointslicemirroring-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:ephemeral-volume-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:expand-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:generic-garbage-collector clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:horizontal-pod-autoscaler clusterrole_rules: [{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*/scale\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\"]},{\"apiGroups\":[\"metrics.k8s.io\"],\"resources\":[\"pods\"],\"verbs\":[\"list\"]},{\"apiGroups\":[\"custom.metrics.k8s.io\"],\"resources\":[\"*\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"external.metrics.k8s.io\"],\"resources\":[\"*\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:job-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:legacy-service-account-token-cleaner clusterrole_rules: [{\"apiGroups\":[\"\"],\"resourceNames\":[\"kube-apiserver-legacy-service-account-token-tracking\"],\"resources\":[\"configmaps\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\",\"patch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:namespace-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces/finalize\",\"namespaces/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"delete\",\"deletecollection\",\"get\",\"list\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:node-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"delete\",\"get\",\"list\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:persistent-volume-binder clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"create\",\"delete\",\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"create\",\"delete\",\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:pod-garbage-collector clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:pv-protection-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:pvc-protection-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:replicaset-controller clusterrole_rules: [{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:replication-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"list\",\"patch\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:resourcequota-controller clusterrole_rules: [{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"resourcequotas/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:root-ca-cert-publisher clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\"],\"verbs\":[\"create\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:route-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:service-account-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:service-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:statefulset-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets/status\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets/finalizers\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"delete\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/finalizers\"],\"verbs\":[\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:ttl-after-finished-controller clusterrole_rules: [{\"apiGroups\":[\"batch\"],\"resources\":[\"jobs\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:ttl-controller clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:controller:validatingadmissionpolicy-status-controller clusterrole_rules: [{\"apiGroups\":[\"admissionregistration.k8s.io\"],\"resources\":[\"validatingadmissionpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"admissionregistration.k8s.io\"],\"resources\":[\"validatingadmissionpolicies/status\"],\"verbs\":[\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:coredns clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\",\"pods\",\"namespaces\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:discovery clusterrole_rules: [{\"nonResourceURLs\":[\"/api\",\"/api/*\",\"/apis\",\"/apis/*\",\"/healthz\",\"/livez\",\"/openapi\",\"/openapi/*\",\"/readyz\",\"/version\",\"/version/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:heapster clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"events\",\"namespaces\",\"nodes\",\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"deployments\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:kube-aggregator clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:kube-controller-manager clusterrole_rules: [{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resourceNames\":[\"kube-controller-manager\"],\"resources\":[\"leases\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\",\"serviceaccounts\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"delete\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"namespaces\",\"secrets\",\"serviceaccounts\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"secrets\",\"serviceaccounts\"],\"verbs\":[\"update\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"*\"],\"resources\":[\"*\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:kube-dns clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:kube-scheduler clusterrole_rules: [{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resourceNames\":[\"kube-scheduler\"],\"resources\":[\"leases\"],\"verbs\":[\"get\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"pods/binding\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"replicationcontrollers\",\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\",\"extensions\"],\"resources\":[\"replicasets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"statefulsets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csistoragecapacities\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:kubelet-api-admin clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"proxy\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/log\",\"nodes/metrics\",\"nodes/proxy\",\"nodes/stats\"],\"verbs\":[\"*\"]}] clusterrole_is_compliant: False",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:monitoring clusterrole_rules: [{\"nonResourceURLs\":[\"/healthz\",\"/healthz/*\",\"/livez\",\"/livez/*\",\"/metrics\",\"/metrics/slis\",\"/readyz\",\"/readyz/*\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:node clusterrole_rules: [{\"apiGroups\":[\"authentication.k8s.io\"],\"resources\":[\"tokenreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"authorization.k8s.io\"],\"resources\":[\"localsubjectaccessreviews\",\"subjectaccessreviews\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"services\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods\"],\"verbs\":[\"create\",\"delete\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/status\"],\"verbs\":[\"patch\",\"update\"]},{\"apiGroups\":[\"\"],\"resources\":[\"pods/eviction\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"secrets\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\",\"persistentvolumes\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"coordination.k8s.io\"],\"resources\":[\"leases\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"volumeattachments\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"serviceaccounts/token\"],\"verbs\":[\"create\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims/status\"],\"verbs\":[\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csidrivers\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"csinodes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"apiGroups\":[\"node.k8s.io\"],\"resources\":[\"runtimeclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:node-bootstrapper clusterrole_rules: [{\"apiGroups\":[\"certificates.k8s.io\"],\"resources\":[\"certificatesigningrequests\"],\"verbs\":[\"create\",\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:node-problem-detector clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes/status\"],\"verbs\":[\"patch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:node-proxier clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"endpoints\",\"services\"],\"verbs\":[\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:persistent-volume-provisioner clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"update\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"events\"],\"verbs\":[\"watch\"]},{\"apiGroups\":[\"\",\"events.k8s.io\"],\"resources\":[\"events\"],\"verbs\":[\"create\",\"patch\",\"update\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:public-info-viewer clusterrole_rules: [{\"nonResourceURLs\":[\"/healthz\",\"/livez\",\"/readyz\",\"/version\",\"/version/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:service-account-issuer-discovery clusterrole_rules: [{\"nonResourceURLs\":[\"/.well-known/openid-configuration\",\"/.well-known/openid-configuration/\",\"/openid/v1/jwks\",\"/openid/v1/jwks/\"],\"verbs\":[\"get\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: system:volume-scheduler clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumes\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"apiGroups\":[\"storage.k8s.io\"],\"resources\":[\"storageclasses\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"persistentvolumeclaims\"],\"verbs\":[\"get\",\"list\",\"patch\",\"update\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            },
            {
                "line": "**clusterrole_name: view clusterrole_rules: [{\"apiGroups\":[\"\"],\"resources\":[\"configmaps\",\"endpoints\",\"persistentvolumeclaims\",\"persistentvolumeclaims/status\",\"pods\",\"replicationcontrollers\",\"replicationcontrollers/scale\",\"serviceaccounts\",\"services\",\"services/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"bindings\",\"events\",\"limitranges\",\"namespaces/status\",\"pods/log\",\"pods/status\",\"replicationcontrollers/status\",\"resourcequotas\",\"resourcequotas/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"\"],\"resources\":[\"namespaces\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"discovery.k8s.io\"],\"resources\":[\"endpointslices\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"apps\"],\"resources\":[\"controllerrevisions\",\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"statefulsets\",\"statefulsets/scale\",\"statefulsets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"autoscaling\"],\"resources\":[\"horizontalpodautoscalers\",\"horizontalpodautoscalers/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"batch\"],\"resources\":[\"cronjobs\",\"cronjobs/status\",\"jobs\",\"jobs/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"extensions\"],\"resources\":[\"daemonsets\",\"daemonsets/status\",\"deployments\",\"deployments/scale\",\"deployments/status\",\"ingresses\",\"ingresses/status\",\"networkpolicies\",\"replicasets\",\"replicasets/scale\",\"replicasets/status\",\"replicationcontrollers/scale\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"policy\"],\"resources\":[\"poddisruptionbudgets\",\"poddisruptionbudgets/status\"],\"verbs\":[\"get\",\"list\",\"watch\"]},{\"apiGroups\":[\"networking.k8s.io\"],\"resources\":[\"ingresses\",\"ingresses/status\",\"networkpolicies\"],\"verbs\":[\"get\",\"list\",\"watch\"]}] clusterrole_is_compliant: True",
                "status": "PASS",
                "reason": "['role_is_compliant'] present as expected; ['clusterrole_is_compliant'] present as expected"
            }
        ],
        "remediation": "Where possible replace any use of wildcards [\"*\"] in roles and clusterroles with specific\nobjects or actions.\nCondition: role_is_compliant is false if [\"*\"] is found in rules.\nCondition: clusterrole_is_compliant is false if [\"*\"] is found in rules.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.4",
        "description": "Minimize access to create pods (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove create access to pod objects in the cluster.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.5",
        "description": "Ensure that default service accounts are not actively used (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "kubectl get serviceaccount --all-namespaces --field-selector metadata.name=default -o=json | jq -r '.items[] | \" namespace: \\(.metadata.namespace), kind: \\(.kind), name: \\(.metadata.name), automountServiceAccountToken: \\(.automountServiceAccountToken | if . == null then \"notset\" else . end )\"' | xargs -L 1\n",
        "audit_output": "namespace: default, kind: ServiceAccount, name: default, automountServiceAccountToken: notset\nnamespace: kube-flannel, kind: ServiceAccount, name: default, automountServiceAccountToken: notset\nnamespace: kube-node-lease, kind: ServiceAccount, name: default, automountServiceAccountToken: notset\nnamespace: kube-public, kind: ServiceAccount, name: default, automountServiceAccountToken: notset\nnamespace: kube-system, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
        "line_results": [
            {
                "line": "namespace: default, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
                "status": "PASS",
                "reason": "['automountServiceAccountToken'] present as expected"
            },
            {
                "line": "namespace: kube-flannel, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
                "status": "PASS",
                "reason": "['automountServiceAccountToken'] present as expected"
            },
            {
                "line": "namespace: kube-node-lease, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
                "status": "PASS",
                "reason": "['automountServiceAccountToken'] present as expected"
            },
            {
                "line": "namespace: kube-public, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
                "status": "PASS",
                "reason": "['automountServiceAccountToken'] present as expected"
            },
            {
                "line": "namespace: kube-system, kind: ServiceAccount, name: default, automountServiceAccountToken: notset",
                "status": "PASS",
                "reason": "['automountServiceAccountToken'] present as expected"
            }
        ],
        "remediation": "Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\n`automountServiceAccountToken: false`.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.6",
        "description": "Ensure that Service Account Tokens are only mounted where necessary (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAMESPACE:.metadata.namespace,POD_NAME:.metadata.name,POD_SERVICE_ACCOUNT:.spec.serviceAccount,POD_IS_AUTOMOUNTSERVICEACCOUNTTOKEN:.spec.automountServiceAccountToken --no-headers | while read -r pod_namespace pod_name pod_service_account pod_is_automountserviceaccounttoken\ndo\n  # Retrieve automountServiceAccountToken's value for ServiceAccount and Pod, set to notset if null or <none>.\n  svacc_is_automountserviceaccounttoken=$(kubectl get serviceaccount -n \"${pod_namespace}\" \"${pod_service_account}\" -o json | jq -r '.automountServiceAccountToken' | sed -e 's/<none>/notset/g' -e 's/null/notset/g')\n  pod_is_automountserviceaccounttoken=$(echo \"${pod_is_automountserviceaccounttoken}\" | sed -e 's/<none>/notset/g' -e 's/null/notset/g')\n  if [ \"${svacc_is_automountserviceaccounttoken}\" = \"false\" ] && ( [ \"${pod_is_automountserviceaccounttoken}\" = \"false\" ] || [ \"${pod_is_automountserviceaccounttoken}\" = \"notset\" ] ); then\n    is_compliant=\"True\"\n  elif [ \"${svacc_is_automountserviceaccounttoken}\" = \"true\" ] && [ \"${pod_is_automountserviceaccounttoken}\" = \"false\" ]; then\n    is_compliant=\"True\"\n  else\n    is_compliant=\"False\"\n  fi\n  echo \"**namespace: ${pod_namespace} pod_name: ${pod_name} service_account: ${pod_service_account} pod_is_automountserviceaccounttoken: ${pod_is_automountserviceaccounttoken} svacc_is_automountServiceAccountToken: ${svacc_is_automountserviceaccounttoken} is_compliant:${is_compliant}\"\ndone\n",
        "audit_output": "**namespace: default pod_name: cis-k8s-audit-xfxgl service_account: audit-runner pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: default pod_name: kube-bench-r2x4j service_account: default pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: kube-flannel pod_name: kube-flannel-ds-5xtkp service_account: flannel pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: kube-system pod_name: coredns-55cb58b774-2zrjw service_account: coredns pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: kube-system pod_name: coredns-55cb58b774-88dz8 service_account: coredns pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: kube-system pod_name: etcd-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False\n**namespace: kube-system pod_name: kube-apiserver-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False\n**namespace: kube-system pod_name: kube-controller-manager-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False\n**namespace: kube-system pod_name: kube-proxy-vhvkv service_account: kube-proxy pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False\n**namespace: kube-system pod_name: kube-scheduler-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False",
        "line_results": [
            {
                "line": "**namespace: default pod_name: cis-k8s-audit-xfxgl service_account: audit-runner pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: default pod_name: kube-bench-r2x4j service_account: default pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-flannel pod_name: kube-flannel-ds-5xtkp service_account: flannel pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: coredns-55cb58b774-2zrjw service_account: coredns pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: coredns-55cb58b774-88dz8 service_account: coredns pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: etcd-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: kube-apiserver-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: kube-controller-manager-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: kube-proxy-vhvkv service_account: kube-proxy pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken: notset is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "**namespace: kube-system pod_name: kube-scheduler-kubernetes service_account: <none> pod_is_automountserviceaccounttoken: notset svacc_is_automountServiceAccountToken:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            }
        ],
        "remediation": "Modify the definition of ServiceAccounts and Pods which do not need to mount service\naccount tokens to disable it, with `automountServiceAccountToken: false`.\nIf both the ServiceAccount and the Pod's .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.\nCondition: Pod is_compliant to true when\n  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset\n  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.7",
        "description": "Avoid use of system:masters group (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Remove the system:masters group from all users in the cluster.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.8",
        "description": "Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove the impersonate, bind and escalate rights from subjects.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.9",
        "description": "Minimize access to create persistent volumes (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove create access to PersistentVolume objects in the cluster.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.10",
        "description": "Minimize access to the proxy sub-resource of nodes (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove access to the proxy sub-resource of node objects.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.11",
        "description": "Minimize access to the approval sub-resource of certificatesigningrequests objects (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove access to the approval sub-resource of certificatesigningrequests objects.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.12",
        "description": "Minimize access to webhook configuration objects (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.1.13",
        "description": "Minimize access to the service account token creation (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Where possible, remove access to the token sub-resource of serviceaccount objects.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.1",
        "description": "Ensure that the cluster has at least one active policy control mechanism in place (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Ensure that either Pod Security Admission or an external policy control system is in place\nfor every namespace which contains user workloads.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.2",
        "description": "Minimize the admission of privileged containers (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o json | jq -c '.spec.containers[]' | while read -r container\n  do\n    # Retrieve container's name.\n    container_name=$(echo ${container} | jq -r '.name')\n    # Retrieve container's .securityContext.privileged value.\n    container_privileged=$(echo ${container} | jq -r '.securityContext.privileged' | sed -e 's/null/notset/g')\n    if [ \"${container_privileged}\" = \"false\" ] || [ \"${container_privileged}\" = \"notset\" ] ; then\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_privileged: ${container_privileged} is_compliant:True\"\n    else\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_privileged: ${container_privileged} is_compliant:False\"\n    fi\n  done\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default is_container_privileged: notset is_compliant:True\n***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default is_container_privileged: notset is_compliant:True\n***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel is_container_privileged: false is_compliant:True\n***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system is_container_privileged: notset is_compliant:True\n***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system is_container_privileged: notset is_compliant:True\n***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system is_container_privileged: notset is_compliant:True\n***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system is_container_privileged: notset is_compliant:True\n***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system is_container_privileged: notset is_compliant:True\n***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system is_container_privileged: true is_compliant:False\n***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel is_container_privileged: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system is_container_privileged: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system is_container_privileged: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of privileged containers.\nAudit: the audit list all pods' containers to retrieve their .securityContext.privileged value.\nCondition: is_compliant is false if container's `.securityContext.privileged` is set to `true`.\nDefault: by default, there are no restrictions on the creation of privileged containers.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.3",
        "description": "Minimize the admission of containers wishing to share the host process ID namespace (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostPID for each pod.\n  pod_hostpid=$(kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o jsonpath='{.spec.hostPID}' 2>/dev/null)\n  if [ -z \"${pod_hostpid}\" ]; then\n    pod_hostpid=\"false\"\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostpid: ${pod_hostpid} is_compliant:True\"\n  else\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostpid: ${pod_hostpid} is_compliant:False\"\n  fi\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostpid: true is_compliant:False\n***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostpid: true is_compliant:False\n***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostpid: false is_compliant:True\n***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostpid: false is_compliant:True\n***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostpid: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostpid: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostpid: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostPID` containers.\nAudit: the audit retrieves each Pod' spec.hostPID.\nCondition: is_compliant is false if Pod's spec.hostPID is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostPID containers.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.4",
        "description": "Minimize the admission of containers wishing to share the host IPC namespace (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostIPC for each pod.\n  pod_hostipc=$(kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o jsonpath='{.spec.hostIPC}' 2>/dev/null)\n  if [ -z \"${pod_hostipc}\" ]; then\n    pod_hostipc=\"false\"\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostipc: ${pod_hostipc} is_compliant:True\"\n  else\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostipc: ${pod_hostipc} is_compliant:False\"\n  fi\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostipc: false is_compliant:True\n***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostipc: false is_compliant:True\n***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostipc: false is_compliant:True\n***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostipc: false is_compliant:True\n***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostipc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostIPC` containers.\nAudit: the audit retrieves each Pod' spec.IPC.\nCondition: is_compliant is false if Pod's spec.hostIPC is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostIPC containers.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.5",
        "description": "Minimize the admission of containers wishing to share the host network namespace (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostNetwork for each pod.\n  pod_hostnetwork=$(kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o jsonpath='{.spec.hostNetwork}' 2>/dev/null)\n  if [ -z \"${pod_hostnetwork}\" ]; then\n    pod_hostnetwork=\"false\"\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostnetwork: ${pod_hostnetwork} is_compliant:True\"\n  else\n    echo \"***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostnetwork: ${pod_hostnetwork} is_compliant:False\"\n  fi\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostnetwork: true is_compliant:False\n***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostnetwork: false is_compliant:True\n***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostnetwork: true is_compliant:False\n***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostnetwork: false is_compliant:True\n***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostnetwork: false is_compliant:True\n***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False\n***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False\n***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False\n***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False\n***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl pod_namespace: default is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j pod_namespace: default is_pod_hostnetwork: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp pod_namespace: kube-flannel is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw pod_namespace: kube-system is_pod_hostnetwork: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 pod_namespace: kube-system is_pod_hostnetwork: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: etcd-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes pod_namespace: kube-system is_pod_hostnetwork: true is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            }
        ],
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostNetwork` containers.\nAudit: the audit retrieves each Pod' spec.hostNetwork.\nCondition: is_compliant is false if Pod's spec.hostNetwork is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostNetwork containers.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.6",
        "description": "Minimize the admission of containers with allowPrivilegeEscalation (Manual)",
        "status": "PASS",
        "reason": "All lines passed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o json | jq -c '.spec.containers[]' | while read -r container\n  do\n    # Retrieve container's name\n    container_name=$(echo ${container} | jq -r '.name')\n    # Retrieve container's .securityContext.allowPrivilegeEscalation\n    container_allowprivesc=$(echo ${container} | jq -r '.securityContext.allowPrivilegeEscalation' | sed -e 's/null/notset/g')\n    if [ \"${container_allowprivesc}\" = \"false\" ] || [ \"${container_allowprivesc}\" = \"notset\" ]; then\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_allowprivesc: ${container_allowprivesc} is_compliant:True\"\n    else\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_allowprivesc: ${container_allowprivesc} is_compliant:False\"\n    fi\n  done\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel is_container_allowprivesc: notset is_compliant:True\n***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system is_container_allowprivesc: false is_compliant:True\n***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system is_container_allowprivesc: false is_compliant:True\n***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True\n***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system is_container_allowprivesc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system is_container_allowprivesc: false is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system is_container_allowprivesc: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `.securityContext.allowPrivilegeEscalation` set to `true`.\nAudit: the audit retrieves each Pod's container(s) `.securityContext.allowPrivilegeEscalation`.\nCondition: is_compliant is false if container's `.securityContext.allowPrivilegeEscalation` is set to `true`.\nDefault: If notset, privilege escalation is allowed (default to true). However if PSP/PSA is used with a `restricted` profile,\nprivilege escalation is explicitly disallowed unless configured otherwise.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.7",
        "description": "Minimize the admission of root containers (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot`\nor `MustRunAs` with the range of UIDs not including 0, is set.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.8",
        "description": "Minimize the admission of containers with the NET_RAW capability (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with the `NET_RAW` capability.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.9",
        "description": "Minimize the admission of containers with added capabilities (Manual)",
        "status": "FAIL",
        "reason": "One or more lines failed",
        "audit_command": "kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \"${pod_name}\" --namespace \"${pod_namespace}\" -o json | jq -c '.spec.containers[]' | while read -r container\n  do\n    # Retrieve container's name\n    container_name=$(echo ${container} | jq -r '.name')\n    # Retrieve container's added capabilities\n    container_caps_add=$(echo ${container} | jq -r '.securityContext.capabilities.add' | sed -e 's/null/notset/g')\n    # Set is_compliant to true by default.\n    is_compliant=true\n    caps_list=\"\"\n    if [ \"${container_caps_add}\" != \"notset\" ]; then\n      # Loop through all caps and append caps_list, then set is_compliant to false.\n      for cap in $(echo \"${container_caps_add}\" | jq -r '.[]'); do\n      caps_list+=\"${cap},\"\n      is_compliant=False\n      done\n      # Remove trailing comma for the last list member.\n      caps_list=${caps_list%,}\n    fi\n    if [ \"${is_compliant}\" = true ]; then\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} container_caps_add: ${container_caps_add} is_compliant:True\"\n    else\n      echo \"***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} container_caps_add: ${caps_list} is_compliant:False\"\n    fi\n  done\ndone\n",
        "audit_output": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default container_caps_add: notset is_compliant:True\n***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default container_caps_add: notset is_compliant:True\n***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel container_caps_add:  is_compliant:False\n***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system container_caps_add:  is_compliant:False\n***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system container_caps_add:  is_compliant:False\n***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system container_caps_add: notset is_compliant:True\n***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system container_caps_add: notset is_compliant:True\n***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system container_caps_add: notset is_compliant:True\n***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system container_caps_add: notset is_compliant:True\n***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system container_caps_add: notset is_compliant:True",
        "line_results": [
            {
                "line": "***pod_name: cis-k8s-audit-xfxgl container_name: check pod_namespace: default container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-bench-r2x4j container_name: kube-bench pod_namespace: default container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-flannel-ds-5xtkp container_name: kube-flannel pod_namespace: kube-flannel container_caps_add:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-2zrjw container_name: coredns pod_namespace: kube-system container_caps_add:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: coredns-55cb58b774-88dz8 container_name: coredns pod_namespace: kube-system container_caps_add:  is_compliant:False",
                "status": "FAIL",
                "reason": "['is_compliant'] != True"
            },
            {
                "line": "***pod_name: etcd-kubernetes container_name: etcd pod_namespace: kube-system container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-apiserver-kubernetes container_name: kube-apiserver pod_namespace: kube-system container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-controller-manager-kubernetes container_name: kube-controller-manager pod_namespace: kube-system container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-proxy-vhvkv container_name: kube-proxy pod_namespace: kube-system container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            },
            {
                "line": "***pod_name: kube-scheduler-kubernetes container_name: kube-scheduler pod_namespace: kube-system container_caps_add: notset is_compliant:True",
                "status": "PASS",
                "reason": "['is_compliant'] == True"
            }
        ],
        "remediation": "Ensure that `allowedCapabilities` is not present in policies for the cluster unless\nit is set to an empty array.\nAudit: the audit retrieves each Pod's container(s) added capabilities.\nCondition: is_compliant is false if added capabilities are added for a given container.\nDefault: Containers run with a default set of capabilities as assigned by the Container Runtime.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.10",
        "description": "Minimize the admission of containers with capabilities assigned (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Review the use of capabilites in applications running on your cluster. Where a namespace\ncontains applications which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.11",
        "description": "Minimize the admission of Windows HostProcess containers (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers that have `.securityContext.windowsOptions.hostProcess` set to `true`.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.12",
        "description": "Minimize the admission of HostPath volumes (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `hostPath` volumes.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.2.13",
        "description": "Minimize the admission of containers which use HostPorts (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers which use `hostPort` sections.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.3.1",
        "description": "Ensure that the CNI in use supports NetworkPolicies (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\nin the Kubernetes cluster.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.3.2",
        "description": "Ensure that all Namespaces have NetworkPolicies defined (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Follow the documentation and create NetworkPolicy objects as you need them.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.4.1",
        "description": "Prefer using Secrets as files over Secrets as environment variables (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "If possible, rewrite application code to read Secrets from mounted secret files, rather than\nfrom environment variables.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.4.2",
        "description": "Consider external secret storage (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Refer to the Secrets management options offered by your cloud provider or a third-party\nsecrets management solution.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.5.1",
        "description": "Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Follow the Kubernetes documentation and setup image provenance.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.6.1",
        "description": "Create administrative boundaries between resources using namespaces (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Follow the documentation and create namespaces for objects in your deployment as you need\nthem.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.6.2",
        "description": "Ensure that the seccomp profile is set to docker/default in your Pod definitions (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Use `securityContext` to enable the docker/default seccomp profile in your pod definitions.\nAn example is as below:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.6.3",
        "description": "Apply SecurityContext to your Pods and Containers (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Follow the Kubernetes documentation and apply SecurityContexts to your Pods. For a\nsuggested list of SecurityContexts, you may refer to the CIS Security Benchmark for Docker\nContainers.",
        "_source_file": "cis-1.11/policies.yaml"
    },
    {
        "check_id": "5.6.4",
        "description": "The default namespace should not be used (Manual)",
        "status": "WARN",
        "reason": "Manual Check, Please verify the recommendation and follow the remediation if needed",
        "remediation": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.",
        "_source_file": "cis-1.11/policies.yaml"
    }
]
